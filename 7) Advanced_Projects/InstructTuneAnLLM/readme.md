Instruction Tuning an LLM makes a base LLM to follow instructions given by humans. This is what makes a GPT model chatgpt. The base model is trained autoregressively ( trained to predict the next token during training). As it is tasked to do so it always tries to output one token at a time during the inference. This token is the one with highest probabilty. 
This is similar to auto complete the sentence. Though the model has learnt from billions and trillions of tokens and gained the understanding of language, it is unsuccessful at following instructions (as it's primary objective is to predict the next token). Hence, instruction finetuning comes to rescue.
Instruction fine tuning is a simple method where we finetune the base LLM on instruction following datasets. The dataset quality is important than the quantity. These are the things to be considered for creating the instruct tuning dataset. The dataset should be very broad covering a wide range of types including question answering , arithematic , conversational etc..
The instruction tuning dataset must follow a template. Here is the template which is used in the notebook. Instruction followed by an optional input followed by response. You can take a look at the format_input() function for more details.
The model is then tasked to predict next token (auto regressive training) but with a slight variation. Instead of finetuning all the parameters of the model which is very tedious, memory consuming we use a very productive and useful technique which is comparable to full fine tuning called parameter efficient fine tuning(PEFT). So we train a fraction of weights/parameters here 1.5% (you can see in the notebook)
After loading the peft model we set all the arguments and take a fraction of dataset (25k out of 430k) and train the model using SFT trainer. I have also created a callback which is executed after logging number of steps. The call back simply outputs the result of original_model or base model , finetuned model/instruction tuned model and the original output(ground truth).
Finally I have trained for 3 epochs and the loss at the end is around 1 and took around 3 hours on 2 T4 GPUs with 15gb . To improve the model performance I would recommend to increase the dataset size from 25k to 100-350k rather than increasing the number of epochs. This would significantly improve the model understanding on varied tasks.