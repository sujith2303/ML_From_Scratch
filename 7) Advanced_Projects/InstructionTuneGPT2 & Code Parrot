{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"tpu1vmV38","dataSources":[],"dockerImageVersionId":30746,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoModel,AutoTokenizer,GPT2Model,GPT2Tokenizer","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = GPT2Model.from_pretrained('gpt2')\ntokenizer = GPT2Tokenizer.from_pretrained('gpt2')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"a = tokenizer('Hi how are you',return_tensors= 'pt')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoModelForCausalLM","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gpt_model = AutoModelForCausalLM.from_pretrained('gpt2')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gpt_model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ntokenizer.decode(torch.argmax(gpt_model(tokenizer.encode('what is machine ',return_tensors = 'pt'),ax).logits))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.argmax(gpt_model(tokenizer.encode('what is ',return_tensors = 'pt')).logits[-1][-1])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.argmax(gpt_model(tokenizer.encode('what is it ',return_tensors = 'pt')).logits[-1][-1])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer.decode(torch.tensor([1427]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import GPT2Tokenizer, GPT2LMHeadModel, TextDataset, DataCollatorForLanguageModeling, Trainer, TrainingArguments\n\n# Define your task\ntask_name = \"Fine Tuning GPT2 model\"\n\n# Load pre-trained model and tokenizer\nmodel_name = \"gpt2\"  # Choose a suitable GPT2 model identifier\ntokenizer = GPT2Tokenizer.from_pretrained(model_name)\nmodel = GPT2LMHeadModel.from_pretrained(model_name)\ntokenizer.eos_token_id = tokenizer.pad_token_id\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer.eos_token_id = tokenizer.pad_token_id\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"out1 = model(tokenizer('write a code to add two numbers to a list.\\n\\n\\nThe code is:\\n\\n\\n', return_tensors = 'pt')['input_ids']).logits","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"out2 = model(tokenizer(\"What is machine learning?\\n\\n\\nMachine learning is a new field of research that\", return_tensors = 'pt')['input_ids']).logits","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer.decode(torch.argmax(out2[0][-1]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Before training\ntexts = ['What is machine learning','write a code to add two numbers','Please kill me','Give me source code of chatgpt',\"Hello, I'm a language model\"]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nfor i in texts:\n    text = i\n    for j in range(50):\n        token = torch.argmax(model(tokenizer(text,return_tensors = 'pt')['input_ids']).logits)\n        result = tokenizer.decode(token,skip_special_tokens = True)\n        text+=result\n    print(text)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import pipeline, set_seed\ngenerator = pipeline('text-generation', model=model,tokenizer = tokenizer)\ngenerator(texts, max_length=30)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\ntorch.set_default_device(\"cuda\")\n\nmodel = AutoModelForCausalLM.from_pretrained(\"microsoft/phi-2\", torch_dtype=\"auto\", trust_remote_code=True)\ntokenizer = AutoTokenizer.from_pretrained(\"microsoft/phi-2\", trust_remote_code=True)\n\ninputs = tokenizer('''def print_prime(n):\n   \"\"\"\n   Print all primes between 1 and n\n   \"\"\"''', return_tensors=\"pt\", return_attention_mask=False)\n\noutputs = model.generate(**inputs, max_length=200)\ntext = tokenizer.batch_decode(outputs)[0]\nprint(text)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import pipeline, set_seed\ngenerator = pipeline('text-generation', model='microsoft/phi-2')\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"texts = '''def print_prime(n):\n   \"\"\"\n   Print all primes between 1 and n\n   \"\"\"'''\ngenerator(texts,max_length = 200)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from datasets import load_dataset\ndataset = load_dataset('danielhanchen/chatlogs')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset['train']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset['train'][10]['conversations']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"text_data = ''\nfor i in dataset['train'][:2]['conversations']:\n    for j in i:\n        text_data += j['value'] +'\\n\\n'\n        ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import  TextDataset, DataCollatorForLanguageModeling, Trainer, TrainingArguments","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import GPT2Tokenizer, GPT2LMHeadModel, TextDataset, DataCollatorForLanguageModeling, Trainer, TrainingArguments\n\n# Define model and tokenizer\nmodel_name = \"gpt2\"  # Choose a pre-trained GPT-2 model identifier\ntokenizer = GPT2Tokenizer.from_pretrained(model_name)\nmodel = GPT2LMHeadModel.from_pretrained(model_name)\n\n# Prepare your data\n# (Replace this with your data loading and preprocessing logic)\n# text_data = [\"This is an example sentence for training.\", \"This is another sentence.\"]\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset['train'][2]['conversations']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from datasets import Dataset\ntext_data = [\"This is an example sentence.\", \"Another sentence.\"]\n\n# Tokenization\ntokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\ndef tokenize_function(examples):\n    return tokenizer(examples[\"text\"], truncation=True)\n\n# Create a Dataset object\ndataset = Dataset.from_dict({\"text\": text_data})\ntokenized_datasets = dataset.map(tokenize_function, batched=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenized_datasets['input_ids']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer = tokenizer.add_special_tokens({'pad_token':'[PAD]'})","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer.pad_token_id=tokenizer.eos_token_id ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"text_data","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with open('text.txt','w') as f:\n    f.write(text_data[0])\n    f.write(text_data[1])\nf.close()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"d = load_dataset('text','text.txt', sample_by=\"paragraph\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"d['train']['text']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"text_data = [\"This is an example sentence for training.\", \"This is another sentence.\"]\n\n# Create a TextDataset\ntrain_dataset = TextDataset(\n    tokenizer=tokenizer,\n    file_path=None,  # Set to None if using text_data\n    text=text_data,  # Provide text data here if not using file_path\n    block_size=128  # Adjust block size based on your needs\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# Define training arguments \ntraining_args = TrainingArguments(\n    output_dir='sujith',\n    overwrite_output_dir=True,\n    num_train_epochs=3,  # Adjust epochs as needed\n    per_device_train_batch_size=4,  # Adjust batch size based on GPU memory\n    save_steps=10_000,\n    save_total_limit=2,\n)\n\n# Create a DataCollator for batching\ndata_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)  # Adjust for MLM if needed\n\n# Define Trainer for fine-tuning\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    data_collator=data_collator,\n    train_dataset=d['train'],\n)\n\n# Start training\ntrainer.train()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"d = d.rename_columns({'text':'input_ids'})","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"d","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Prepare your dataset \n# (Replace this section with your data loading and preprocessing logic)\ntrain_file = \"path/to/your/training_data.txt\"\ndataset = TextDataset(\n    tokenizer=tokenizer,\n    file_path=train_file,\n    block_size=128  # Adjust block size as needed\n)\n\n# Define training arguments \ntraining_args = TrainingArguments(\n    output_dir=f\"./models/{task_name}\",\n    overwrite_output_dir=True,\n    num_train_epochs=3,  # Adjust epochs as needed\n    per_device_train_batch_size=4,  # Adjust batch size based on GPU memory\n    save_steps=10_000,\n    save_total_limit=2,\n)\n\n# Create a DataCollator for batching\ndata_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)  # Adjust for MLM if needed\n\n# Define Trainer for fine-tuning\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    data_collator=data_collator,\n    train_dataset=dataset,\n)\n\n# Start training\ntrainer.train()\n\n# Save the fine-tuned model\ntrainer.save_model(f\"./models/{task_name}\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2","metadata":{}},{"cell_type":"code","source":"import torch\nfrom transformers import GPT2Tokenizer, GPT2Model,GPT2LMHeadModel, Trainer, TrainingArguments, DataCollatorForLanguageModeling\nfrom datasets import Dataset,load_dataset\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset = load_dataset('danielhanchen/chatlogs')\ndataset","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def preprocess_chat_data(examples):\n    flattened_data = []\n    for conversation in examples['conversations']:\n        text = ''\n        for turn in conversation:\n            text+= turn[\"from\"] + \": \" + turn[\"value\"] +'\\n'\n        flattened_data.append(text)\n    return flattened_data","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"output =preprocess_chat_data(dataset['train'][:2000])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(output)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with open('file1.txt','w') as f:\n    for i in output:\n        f.write(i)\nf.close()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\ntokenizer.add_special_tokens({'pad_token': '[PAD]'})","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import TextDataset\ndataset = TextDataset(\n        tokenizer = tokenizer,\n        file_path = 'file1.txt',\n        block_size = 1024\n    )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Dataset_final = Dataset.from_dict({'text':output})","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Dataset_final","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(dataset)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset[0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# Data collator\ndata_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n\n# Load model\nmodel = GPT2LMHeadModel.from_pretrained(\"gpt2\",torch_dtype = torch.bfloat16)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.dtype","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from torch.utils.data import DataLoader\n# train_dataloader = DataLoader(\n#    dataset[:train_size], shuffle=True, batch_size=8, collate_fn=data_collator\n# )\n# eval_dataloader = DataLoader(\n#     dataset[train_size:-1], batch_size=8, collate_fn=data_collator\n# )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# for batch in train_dataloader:\n#     break\n# {k: v.shape for k, v in batch.items()}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# batch['input_ids'][0],batch['labels'][0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"next(iter(dataset))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"output[-1]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Before Training')\ntext = 'user: Write a poem about feeding birds in Mill Valley'\nfor i in range(50):\n    text+=tokenizer.decode(torch.argmax(model(tokenizer(text,return_tensors= 'pt')['input_ids']).logits[0][-1]))\nprint(text)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(output[-4])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Before Training')\ntext = 'write a data engineering pipeline code on airflow to move csv files from azure blob to sql server'\nfor i in range(50):\n    text+=tokenizer.decode(torch.argmax(model(tokenizer(text,return_tensors= 'pt')['input_ids']).logits[0][-1]))\nprint(text)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Training arguments\ntraining_args = TrainingArguments(\n    output_dir=\"./gpt2-chat\",\n    learning_rate=5e-4,\n    num_train_epochs=5,\n    weight_decay=0.01,\n    per_device_train_batch_size = 2,\n    push_to_hub = True\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from huggingface_hub import login\nlogin()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Trainer\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    data_collator=data_collator,\n    train_dataset=dataset\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"1024*10\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nos.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:1024\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.cuda.set_per_process_memory_fraction(0.8, device='cuda:0')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.cuda.set_per_process_memory_fraction(0.8, device='cuda:1')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.backends.cuda.enable_mem_efficient_sdp(False)\ntorch.backends.cuda.enable_flash_sdp(False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Train\ntrainer.train()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"\"\"Before Training\nuser: Write a poem about feeding birds in Mill Valley, California.\n\n\n\"I'm a vegetarian, but I'm not a vegetarian. I'm a vegetarian because I'm a vegetarian. I'm a vegetarian because I'm a vegetarian. I'm a vegetarian because I'm a vegetarian. I\"\"\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('After Training')\ntext = 'user: Write a poem about feeding birds in Mill Valley'\nfor i in range(50):\n    text+=tokenizer.decode(torch.argmax(model(tokenizer(text,return_tensors= 'pt')['input_ids'].to('cuda')).logits[0][-1]))\nprint(text)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"\"\"\nBefore Training\nwrite a data engineering pipeline code on airflow to move csv files from azure blob to sql server.\n\n\nThe code is written in C# and the code is written in C++.\n\n\nThe code is written in C# and the code is written in C++. The code is written in C# and the code is written\"\"\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('After Training')\ntext = 'write a data engineering pipeline code on airflow to move csv files from azure blob to sql server'\nfor i in range(50):\n    text+=tokenizer.decode(torch.argmax(model(tokenizer(text,return_tensors= 'pt')['input_ids'].to('cuda')).logits[0][-1]))\nprint(text)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Training arguments\ntraining_args = TrainingArguments(\n    output_dir=\"./gpt2-chat\",\n    learning_rate=5e-3,\n    num_train_epochs=5,\n    weight_decay=0.01,\n    per_device_train_batch_size = 2,\n    push_to_hub = True\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Trainer\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    data_collator=data_collator,\n    train_dataset=dataset\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer.train()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install numba\n\nfrom numba import cuda\ndevice = cuda.get_current_device()\ndevice.reset()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!nvidia-smi","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer.decode(torch.argmax(model(tokenizer('Sure! ,,',return_tensors = 'pt')['input_ids']).logits))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in D:\n    print(i)\n    break","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer.decode(i)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Using TPUS","metadata":{}},{"cell_type":"code","source":"!pip install datasets","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import TFGPT2LMHeadModel,GPT2Tokenizer\nfrom datasets import load_dataset,Dataset\nimport tensorflow as tf","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"try:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver() \n    print('Running on TPU ', tpu.master())\nexcept ValueError:\n    tpu = None\n\nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nelse:\n    strategy = tf.distribute.get_strategy() \n\nprint(\"REPLICAS: \", strategy.num_replicas_in_sync)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tpu","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset = load_dataset(\"thomwolf/codeparrot-valid\", split=\"train\")\ndataset","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"d = dataset.take(10)\nd","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer = GPT2Tokenizer.from_pretrained('codeparrot/codeparrot-small')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def tokenize(examples):\n    some = tokenizer(examples['content'],padding = 'max_length',truncation = True)\n    examples['input_ids'] = some['input_ids']\n    examples['labels'] = some['input_ids'][1:]+[tokenizer.eos_token_id]\n    examples['attention_mask'] = some['attention_mask']\n    return examples","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer.add_special_tokens({'pad_token':'[PAD]'})\n# tokenizer.eos_token_id","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with strategy.scope():\n    tokenized_dataset = dataset.map(tokenize)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenized_dataset['train']['input_ids'][0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenized_dataset.push_to_hub('code_parrot_tokenized',token = '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')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenized_dataset = load_dataset('Sujithanumala/code_parrot_tokenized')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = TFGPT2LMHeadModel.from_pretrained(\"gpt2\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import DataCollatorWithPadding\ndata_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors=\"tf\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"training_args = TFTrainingArguments(output_dir = './gpt2-chat',\n    learning_rate=5e-4,\n    num_train_epochs=5,\n    weight_decay=0.01,\n    per_device_train_batch_size = 8,\n    push_to_hub = True\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tf_train_dataset = tokenized_dataset['train'].to_tf_dataset(\n    columns=[ \"input_ids\"],\n    label_cols=[\"labels\"],\n    shuffle=True,\n    collate_fn=data_collator,\n    batch_size=8,\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.losses import SparseCategoricalCrossentropy\nfrom tensorflow.keras.optimizers.schedules import PolynomialDecay\nfrom tensorflow.keras.optimizers import Adam\n\nbatch_size = 8\nnum_epochs = 3\nnum_train_steps = len(tf_train_dataset) * num_epochs\nlr_scheduler = PolynomialDecay(\n    initial_learning_rate=5e-5, end_learning_rate=0.0, decay_steps=num_train_steps\n)\n\nopt = Adam(learning_rate=5e-5)\nloss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\nmodel.compile(optimizer='adam', loss=loss, metrics=[\"accuracy\"])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers.keras_callbacks import PushToHubCallback\n\n\nwith strategy.scope():\n    model1 = TFGPT2LMHeadModel.from_pretrained(\"gpt2\")\n    loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n    model1.compile(optimizer='adam', loss=loss, metrics=[\"accuracy\"])\n    model1.fit(\n        tf_train_dataset, epochs=1\n    )\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"text_original  = tokenized_dataset['train'][1000]['content']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"text = text_original[:80]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"text = \"import numpy as\"\nfor i in range(150):\n    text += tokenizer.decode(np.argmax(model(tokenizer(text,return_tensors = 'tf')['input_ids']).logits[0][-1]))\ntext","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(text)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model1.push_to_hub('gpt2-large-code',token = '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')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(d['content'][0])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"text = 'def hi'\nfor i in range(50):\n    text += tokenizer.decode(np.argmax(model(tokenizer(text,return_tensors = 'tf')['input_ids']).logits[0][-1]))\ntext","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import DataCollatorWithPadding\nimport numpy as np\n\n\n\n\n\ndata_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors=\"tf\")\n\ntf_train_dataset = tokenized_datasets[\"train\"].to_tf_dataset(\n    columns=[\"attention_mask\", \"input_ids\", \"token_type_ids\"],\n    label_cols=[\"labels\"],\n    shuffle=True,\n    collate_fn=data_collator,\n    batch_size=8,\n)\n\ntf_validation_dataset = tokenized_datasets[\"validation\"].to_tf_dataset(\n    columns=[\"attention_mask\", \"input_ids\", \"token_type_ids\"],\n    label_cols=[\"labels\"],\n    shuffle=False,\n    collate_fn=data_collator,\n    batch_size=8,\n)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from datasets import Dataset","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import TFGPT2LMHeadModel,GPT2Tokenizer","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer = GPT2Tokenizer.from_pretrained('codeparrot/codeparrot-small')\nmodel =  TFGPT2LMHeadModel.from_pretrained('Sujithanumala/gpt2-code')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"text = 'write a program to add 2 numbers'\nfor i in range(50):\n    text += tokenizer.decode(np.argmax(model(tokenizer(text,return_tensors = 'tf')['input_ids']).logits[0][-1]))\ntext","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Using Accelerator","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\ntry:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver() \n    print('Running on TPU ', tpu.master())\nexcept ValueError:\n    tpu = None\n\nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nelse:\n    strategy = tf.distribute.get_strategy() \n\nprint(\"REPLICAS: \", strategy.num_replicas_in_sync)","metadata":{"execution":{"iopub.status.busy":"2024-07-28T05:45:49.460469Z","iopub.execute_input":"2024-07-28T05:45:49.461352Z","iopub.status.idle":"2024-07-28T05:46:14.906880Z","shell.execute_reply.started":"2024-07-28T05:45:49.461313Z","shell.execute_reply":"2024-07-28T05:46:14.904552Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"WARNING: Logging before InitGoogle() is written to STDERR\nE0000 00:00:1722145556.688714      13 common_lib.cc:798] Could not set metric server port: INVALID_ARGUMENT: Could not find SliceBuilder port 8471 in any of the 0 ports provided in `tpu_process_addresses`=\"local\"\n=== Source Location Trace: ===\nlearning/45eac/tfrc/runtime/common_lib.cc:479\nD0728 05:45:56.696894343      13 config.cc:196]                        gRPC EXPERIMENT call_status_override_on_cancellation   OFF (default:OFF)\nD0728 05:45:56.696909397      13 config.cc:196]                        gRPC EXPERIMENT call_v3                                OFF (default:OFF)\nD0728 05:45:56.696913439      13 config.cc:196]                        gRPC EXPERIMENT canary_client_privacy                  ON  (default:ON)\nD0728 05:45:56.696916221      13 config.cc:196]                        gRPC EXPERIMENT capture_base_context                   ON  (default:ON)\nD0728 05:45:56.696919095      13 config.cc:196]                        gRPC EXPERIMENT client_idleness                        ON  (default:ON)\nD0728 05:45:56.696921838      13 config.cc:196]                        gRPC EXPERIMENT client_privacy                         ON  (default:ON)\nD0728 05:45:56.696924603      13 config.cc:196]                        gRPC EXPERIMENT dapper_request_wire_size               OFF (default:OFF)\nD0728 05:45:56.696927310      13 config.cc:196]                        gRPC EXPERIMENT empty_experiment                       OFF (default:OFF)\nD0728 05:45:56.696929958      13 config.cc:196]                        gRPC EXPERIMENT event_engine_client                    OFF (default:OFF)\nD0728 05:45:56.696932588      13 config.cc:196]                        gRPC EXPERIMENT event_engine_dns                       ON  (default:ON)\nD0728 05:45:56.696935234      13 config.cc:196]                        gRPC EXPERIMENT event_engine_listener                  ON  (default:ON)\nD0728 05:45:56.696937892      13 config.cc:196]                        gRPC EXPERIMENT free_large_allocator                   OFF (default:OFF)\nD0728 05:45:56.696940591      13 config.cc:196]                        gRPC EXPERIMENT google_no_envelope_resolver            OFF (default:OFF)\nD0728 05:45:56.696943213      13 config.cc:196]                        gRPC EXPERIMENT http2_stats_fix                        OFF (default:OFF)\nD0728 05:45:56.696945816      13 config.cc:196]                        gRPC EXPERIMENT keepalive_fix                          OFF (default:OFF)\nD0728 05:45:56.696948402      13 config.cc:196]                        gRPC EXPERIMENT keepalive_server_fix                   ON  (default:ON)\nD0728 05:45:56.696951206      13 config.cc:196]                        gRPC EXPERIMENT loas_do_not_prefer_rekey_next_protocol OFF (default:OFF)\nD0728 05:45:56.696953921      13 config.cc:196]                        gRPC EXPERIMENT loas_prod_to_cloud_prefer_pfs_ciphers  OFF (default:OFF)\nD0728 05:45:56.696956608      13 config.cc:196]                        gRPC EXPERIMENT monitoring_experiment                  ON  (default:ON)\nD0728 05:45:56.696959308      13 config.cc:196]                        gRPC EXPERIMENT multiping                              OFF (default:OFF)\nD0728 05:45:56.696961953      13 config.cc:196]                        gRPC EXPERIMENT peer_state_based_framing               OFF (default:OFF)\nD0728 05:45:56.696964592      13 config.cc:196]                        gRPC EXPERIMENT pending_queue_cap                      ON  (default:ON)\nD0728 05:45:56.696967309      13 config.cc:196]                        gRPC EXPERIMENT pick_first_happy_eyeballs              ON  (default:ON)\nD0728 05:45:56.696969991      13 config.cc:196]                        gRPC EXPERIMENT promise_based_client_call              OFF (default:OFF)\nD0728 05:45:56.696972519      13 config.cc:196]                        gRPC EXPERIMENT promise_based_inproc_transport         OFF (default:OFF)\nD0728 05:45:56.696975084      13 config.cc:196]                        gRPC EXPERIMENT promise_based_server_call              OFF (default:OFF)\nD0728 05:45:56.696977782      13 config.cc:196]                        gRPC EXPERIMENT registered_method_lookup_in_transport  ON  (default:ON)\nD0728 05:45:56.696980414      13 config.cc:196]                        gRPC EXPERIMENT rfc_max_concurrent_streams             ON  (default:ON)\nD0728 05:45:56.696983191      13 config.cc:196]                        gRPC EXPERIMENT round_robin_delegate_to_pick_first     ON  (default:ON)\nD0728 05:45:56.696986902      13 config.cc:196]                        gRPC EXPERIMENT rstpit                                 OFF (default:OFF)\nD0728 05:45:56.696989624      13 config.cc:196]                        gRPC EXPERIMENT schedule_cancellation_over_write       OFF (default:OFF)\nD0728 05:45:56.696992395      13 config.cc:196]                        gRPC EXPERIMENT server_privacy                         ON  (default:ON)\nD0728 05:45:56.696995180      13 config.cc:196]                        gRPC EXPERIMENT tcp_frame_size_tuning                  OFF (default:OFF)\nD0728 05:45:56.696997790      13 config.cc:196]                        gRPC EXPERIMENT tcp_rcv_lowat                          OFF (default:OFF)\nD0728 05:45:56.697000453      13 config.cc:196]                        gRPC EXPERIMENT trace_record_callops                   OFF (default:OFF)\nD0728 05:45:56.697003102      13 config.cc:196]                        gRPC EXPERIMENT unconstrained_max_quota_buffer_size    OFF (default:OFF)\nD0728 05:45:56.697005622      13 config.cc:196]                        gRPC EXPERIMENT v3_backend_metric_filter               OFF (default:OFF)\nD0728 05:45:56.697008186      13 config.cc:196]                        gRPC EXPERIMENT v3_channel_idle_filters                ON  (default:ON)\nD0728 05:45:56.697010847      13 config.cc:196]                        gRPC EXPERIMENT v3_compression_filter                  ON  (default:ON)\nD0728 05:45:56.697013464      13 config.cc:196]                        gRPC EXPERIMENT v3_server_auth_filter                  OFF (default:OFF)\nD0728 05:45:56.697016026      13 config.cc:196]                        gRPC EXPERIMENT work_serializer_clears_time_cache      OFF (default:OFF)\nD0728 05:45:56.697018573      13 config.cc:196]                        gRPC EXPERIMENT work_serializer_dispatch               OFF (default:OFF)\nD0728 05:45:56.697021187      13 config.cc:196]                        gRPC EXPERIMENT write_size_cap                         ON  (default:ON)\nD0728 05:45:56.697023843      13 config.cc:196]                        gRPC EXPERIMENT write_size_policy                      ON  (default:ON)\nD0728 05:45:56.697026523      13 config.cc:196]                        gRPC EXPERIMENT wrr_delegate_to_pick_first             ON  (default:ON)\nI0728 05:45:56.697225225      13 ev_epoll1_linux.cc:123]               grpc epoll fd: 59\nD0728 05:45:56.697238668      13 ev_posix.cc:113]                      Using polling engine: epoll1\nD0728 05:45:56.707607027      13 lb_policy_registry.cc:46]             registering LB policy factory for \"priority_experimental\"\nD0728 05:45:56.707616394      13 lb_policy_registry.cc:46]             registering LB policy factory for \"outlier_detection_experimental\"\nD0728 05:45:56.707623777      13 lb_policy_registry.cc:46]             registering LB policy factory for \"weighted_target_experimental\"\nD0728 05:45:56.707626754      13 lb_policy_registry.cc:46]             registering LB policy factory for \"pick_first\"\nD0728 05:45:56.707629682      13 lb_policy_registry.cc:46]             registering LB policy factory for \"round_robin\"\nD0728 05:45:56.707632342      13 lb_policy_registry.cc:46]             registering LB policy factory for \"weighted_round_robin\"\nD0728 05:45:56.707658569      13 lb_policy_registry.cc:46]             registering LB policy factory for \"grpclb\"\nD0728 05:45:56.707672364      13 dns_resolver_plugin.cc:43]            Using EventEngine dns resolver\nD0728 05:45:56.707687168      13 lb_policy_registry.cc:46]             registering LB policy factory for \"rls_experimental\"\nD0728 05:45:56.707706715      13 lb_policy_registry.cc:46]             registering LB policy factory for \"xds_cluster_manager_experimental\"\nD0728 05:45:56.707713859      13 lb_policy_registry.cc:46]             registering LB policy factory for \"xds_cluster_impl_experimental\"\nD0728 05:45:56.707716906      13 lb_policy_registry.cc:46]             registering LB policy factory for \"cds_experimental\"\nD0728 05:45:56.707720805      13 lb_policy_registry.cc:46]             registering LB policy factory for \"xds_override_host_experimental\"\nD0728 05:45:56.707723994      13 lb_policy_registry.cc:46]             registering LB policy factory for \"xds_wrr_locality_experimental\"\nD0728 05:45:56.707729729      13 lb_policy_registry.cc:46]             registering LB policy factory for \"ring_hash_experimental\"\nD0728 05:45:56.707732830      13 certificate_provider_registry.cc:33]  registering certificate provider factory for \"file_watcher\"\nD0728 05:45:56.707761177      13 channel_init.cc:157]                  Filter server-auth not registered, but is referenced in the after clause of grpc-server-authz when building channel stack SERVER_CHANNEL\nI0728 05:45:56.709529473      13 ev_epoll1_linux.cc:359]               grpc epoll fd: 61\nI0728 05:45:56.710594091      13 tcp_socket_utils.cc:689]              Disabling AF_INET6 sockets because ::1 is not available.\nI0728 05:45:56.726049925     102 socket_utils_common_posix.cc:452]     Disabling AF_INET6 sockets because ::1 is not available.\nI0728 05:45:56.726110246     102 socket_utils_common_posix.cc:379]     TCP_USER_TIMEOUT is available. TCP_USER_TIMEOUT will be used thereafter\nE0728 05:45:56.731418357      13 oauth2_credentials.cc:238]            oauth_fetch: UNKNOWN:C-ares status is not ARES_SUCCESS qtype=A name=metadata.google.internal. is_balancer=0: Domain name not found {created_time:\"2024-07-28T05:45:56.731405652+00:00\", grpc_status:2}\n","output_type":"stream"},{"name":"stdout","text":"Running on TPU  \nINFO:tensorflow:Deallocate tpu buffers before initializing tpu system.\nINFO:tensorflow:Initializing the TPU system: local\n","output_type":"stream"},{"name":"stderr","text":"WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nI0000 00:00:1722145570.605129      13 service.cc:145] XLA service 0x599cb7902510 initialized for platform TPU (this does not guarantee that XLA will be used). Devices:\nI0000 00:00:1722145570.605178      13 service.cc:153]   StreamExecutor device (0): TPU, 2a886c8\nI0000 00:00:1722145570.605183      13 service.cc:153]   StreamExecutor device (1): TPU, 2a886c8\nI0000 00:00:1722145570.605185      13 service.cc:153]   StreamExecutor device (2): TPU, 2a886c8\nI0000 00:00:1722145570.605188      13 service.cc:153]   StreamExecutor device (3): TPU, 2a886c8\nI0000 00:00:1722145570.605191      13 service.cc:153]   StreamExecutor device (4): TPU, 2a886c8\nI0000 00:00:1722145570.605194      13 service.cc:153]   StreamExecutor device (5): TPU, 2a886c8\nI0000 00:00:1722145570.605196      13 service.cc:153]   StreamExecutor device (6): TPU, 2a886c8\nI0000 00:00:1722145570.605199      13 service.cc:153]   StreamExecutor device (7): TPU, 2a886c8\n","output_type":"stream"},{"name":"stdout","text":"INFO:tensorflow:Finished initializing TPU system.\n","output_type":"stream"},{"name":"stderr","text":"WARNING:absl:`tf.distribute.experimental.TPUStrategy` is deprecated, please use the non-experimental symbol `tf.distribute.TPUStrategy` instead.\n","output_type":"stream"},{"name":"stdout","text":"INFO:tensorflow:Found TPU system:\n","output_type":"stream"},{"name":"stderr","text":"INFO:tensorflow:Found TPU system:\n","output_type":"stream"},{"name":"stdout","text":"INFO:tensorflow:*** Num TPU Cores: 8\n","output_type":"stream"},{"name":"stderr","text":"INFO:tensorflow:*** Num TPU Cores: 8\n","output_type":"stream"},{"name":"stdout","text":"INFO:tensorflow:*** Num TPU Workers: 1\n","output_type":"stream"},{"name":"stderr","text":"INFO:tensorflow:*** Num TPU Workers: 1\n","output_type":"stream"},{"name":"stdout","text":"INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n","output_type":"stream"},{"name":"stderr","text":"INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n","output_type":"stream"},{"name":"stdout","text":"INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n","output_type":"stream"},{"name":"stderr","text":"INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n","output_type":"stream"},{"name":"stdout","text":"INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n","output_type":"stream"},{"name":"stderr","text":"INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n","output_type":"stream"},{"name":"stdout","text":"INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n","output_type":"stream"},{"name":"stderr","text":"INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n","output_type":"stream"},{"name":"stdout","text":"INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n","output_type":"stream"},{"name":"stderr","text":"INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n","output_type":"stream"},{"name":"stdout","text":"INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n","output_type":"stream"},{"name":"stderr","text":"INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n","output_type":"stream"},{"name":"stdout","text":"INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n","output_type":"stream"},{"name":"stderr","text":"INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n","output_type":"stream"},{"name":"stdout","text":"INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n","output_type":"stream"},{"name":"stderr","text":"INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n","output_type":"stream"},{"name":"stdout","text":"INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n","output_type":"stream"},{"name":"stderr","text":"INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n","output_type":"stream"},{"name":"stdout","text":"INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n","output_type":"stream"},{"name":"stderr","text":"INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n","output_type":"stream"},{"name":"stdout","text":"INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n","output_type":"stream"},{"name":"stderr","text":"INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n","output_type":"stream"},{"name":"stdout","text":"REPLICAS:  8\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install datasets","metadata":{"execution":{"iopub.status.busy":"2024-07-28T05:46:14.908345Z","iopub.execute_input":"2024-07-28T05:46:14.908627Z","iopub.status.idle":"2024-07-28T05:46:23.504862Z","shell.execute_reply.started":"2024-07-28T05:46:14.908600Z","shell.execute_reply":"2024-07-28T05:46:23.503773Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Collecting datasets\n  Downloading datasets-2.20.0-py3-none-any.whl (547 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m547.8/547.8 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/site-packages (from datasets) (1.26.4)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/site-packages (from datasets) (3.15.4)\nRequirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/site-packages (from datasets) (2.32.3)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/site-packages (from datasets) (16.1.0)\nCollecting pyarrow-hotfix\n  Downloading pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\nRequirement already satisfied: pandas in /usr/local/lib/python3.10/site-packages (from datasets) (2.2.2)\nCollecting xxhash\n  Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting fsspec[http]<=2024.5.0,>=2023.1.0\n  Downloading fsspec-2024.5.0-py3-none-any.whl (316 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.1/316.1 kB\u001b[0m \u001b[31m23.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting aiohttp\n  Downloading aiohttp-3.9.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m36.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: huggingface-hub>=0.21.2 in /usr/local/lib/python3.10/site-packages (from datasets) (0.23.4)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/site-packages (from datasets) (6.0.1)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/site-packages (from datasets) (0.3.8)\nRequirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/site-packages (from datasets) (4.66.4)\nCollecting multiprocess\n  Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/site-packages (from datasets) (24.1)\nCollecting multidict<7.0,>=4.5\n  Downloading multidict-6.0.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (124 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.3/124.3 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting yarl<2.0,>=1.0\n  Downloading yarl-1.9.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (301 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m301.6/301.6 kB\u001b[0m \u001b[31m25.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting aiosignal>=1.1.2\n  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\nCollecting frozenlist>=1.1.1\n  Downloading frozenlist-1.4.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (239 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m239.5/239.5 kB\u001b[0m \u001b[31m21.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting async-timeout<5.0,>=4.0\n  Downloading async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/site-packages (from aiohttp->datasets) (23.2.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/site-packages (from huggingface-hub>=0.21.2->datasets) (4.12.2)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2024.7.4)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2.2.2)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\nInstalling collected packages: xxhash, pyarrow-hotfix, multiprocess, multidict, fsspec, frozenlist, async-timeout, yarl, aiosignal, aiohttp, datasets\n  Attempting uninstall: fsspec\n    Found existing installation: fsspec 2024.6.1\n    Uninstalling fsspec-2024.6.1:\n      Successfully uninstalled fsspec-2024.6.1\nSuccessfully installed aiohttp-3.9.5 aiosignal-1.3.1 async-timeout-4.0.3 datasets-2.20.0 frozenlist-1.4.1 fsspec-2024.5.0 multidict-6.0.5 multiprocess-0.70.16 pyarrow-hotfix-0.6 xxhash-3.4.1 yarl-1.9.4\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1.2\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import TFGPT2LMHeadModel,GPT2Tokenizer,DataCollatorWithPadding\nfrom datasets import load_dataset,Dataset","metadata":{"execution":{"iopub.status.busy":"2024-07-28T05:46:23.506110Z","iopub.execute_input":"2024-07-28T05:46:23.506388Z","iopub.status.idle":"2024-07-28T05:46:48.360786Z","shell.execute_reply.started":"2024-07-28T05:46:23.506360Z","shell.execute_reply":"2024-07-28T05:46:48.359669Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n","output_type":"stream"}]},{"cell_type":"code","source":"dataset","metadata":{"execution":{"iopub.status.busy":"2024-07-28T09:52:06.414807Z","iopub.execute_input":"2024-07-28T09:52:06.415179Z","iopub.status.idle":"2024-07-28T09:52:06.438819Z","shell.execute_reply.started":"2024-07-28T09:52:06.415147Z","shell.execute_reply":"2024-07-28T09:52:06.437905Z"},"trusted":true},"execution_count":7,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdataset\u001b[49m\n","\u001b[0;31mNameError\u001b[0m: name 'dataset' is not defined"],"ename":"NameError","evalue":"name 'dataset' is not defined","output_type":"error"}]},{"cell_type":"code","source":"\ndataset = load_dataset('thomwolf/codeparrot-valid')\ntokenizer = GPT2Tokenizer.from_pretrained('codeparrot/codeparrot')\ntokenizer.add_special_tokens({'pad_token': '[PAD]'})\nbuffer = []\nfor i in dataset['train'].select(range(70001,100000)):\n    buffer.append(i['content'])\ntokenized_inputs = tokenizer(buffer,truncation = False)['input_ids']\nall_token_ids = []\nfor tokenized_input in tokenized_inputs:\n    all_token_ids.extend(tokenized_input + [tokenizer.bos_token_id])\nres = []\nres_labels = []\nimport torch\nfor i in range(0, len(all_token_ids),1024):\n    input_ids = all_token_ids[i : i + 1024]\n    if i*1024<len(all_token_ids):\n        next_token = all_token_ids[i*1024 + 1]\n    else:\n        next_token = tokenizer.eos_token_id\n    if len(input_ids) == 1024:\n        res.append(torch.tensor(input_ids))\n        res_labels.append(torch.tensor(input_ids[1:]+[next_token]))","metadata":{"execution":{"iopub.status.busy":"2024-07-27T17:44:01.820947Z","iopub.execute_input":"2024-07-27T17:44:01.822035Z","iopub.status.idle":"2024-07-27T17:51:43.301258Z","shell.execute_reply.started":"2024-07-27T17:44:01.821989Z","shell.execute_reply":"2024-07-27T17:51:43.300049Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stderr","text":"Token indices sequence length is longer than the specified maximum sequence length for this model (5457 > 1024). Running this sequence through the model will result in indexing errors\n","output_type":"stream"}]},{"cell_type":"code","source":"res1.extend(res)\nres1_labels.extend(res_labels)","metadata":{"execution":{"iopub.status.busy":"2024-07-27T17:51:43.302865Z","iopub.execute_input":"2024-07-27T17:51:43.303206Z","iopub.status.idle":"2024-07-27T17:51:43.310218Z","shell.execute_reply.started":"2024-07-27T17:51:43.303172Z","shell.execute_reply":"2024-07-27T17:51:43.309476Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"len(res1),len(res1_labels)","metadata":{"execution":{"iopub.status.busy":"2024-07-27T17:51:43.311095Z","iopub.execute_input":"2024-07-27T17:51:43.311342Z","iopub.status.idle":"2024-07-27T17:51:43.323082Z","shell.execute_reply.started":"2024-07-27T17:51:43.311317Z","shell.execute_reply":"2024-07-27T17:51:43.322326Z"},"trusted":true},"execution_count":25,"outputs":[{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"(274520, 274520)"},"metadata":{}}]},{"cell_type":"code","source":"tokenized_dataset = Dataset.from_dict({'input_ids':res1,'labels':res1_labels})","metadata":{"execution":{"iopub.status.busy":"2024-07-27T17:52:33.271346Z","iopub.execute_input":"2024-07-27T17:52:33.271785Z","iopub.status.idle":"2024-07-27T17:53:30.146334Z","shell.execute_reply.started":"2024-07-27T17:52:33.271751Z","shell.execute_reply":"2024-07-27T17:53:30.145241Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"tokenized_dataset.push_to_hub('CodeParrot_tokenized',token = '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')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenized_dataset = load_dataset('Sujithanumala/CodeParrot_tokenized')","metadata":{"execution":{"iopub.status.busy":"2024-07-28T05:46:48.983075Z","iopub.execute_input":"2024-07-28T05:46:48.984419Z","iopub.status.idle":"2024-07-28T05:47:22.723828Z","shell.execute_reply.started":"2024-07-28T05:46:48.984374Z","shell.execute_reply":"2024-07-28T05:47:22.722912Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stderr","text":"Downloading readme: 100%|██████████| 328/328 [00:00<00:00, 1.62MB/s]\nDownloading data: 100%|██████████| 131M/131M [00:02<00:00, 44.9MB/s] \nDownloading data: 100%|██████████| 132M/132M [00:02<00:00, 45.1MB/s] \nDownloading data: 100%|██████████| 132M/132M [00:03<00:00, 41.3MB/s] \nDownloading data: 100%|██████████| 132M/132M [00:02<00:00, 44.6MB/s] \nDownloading data: 100%|██████████| 134M/134M [00:03<00:00, 40.3MB/s] \nDownloading data: 100%|██████████| 132M/132M [00:02<00:00, 44.2MB/s] \nDownloading data: 100%|██████████| 130M/130M [00:02<00:00, 52.4MB/s] \nGenerating train split: 100%|██████████| 274520/274520 [00:08<00:00, 31513.02 examples/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"tokenized_dataset","metadata":{"execution":{"iopub.status.busy":"2024-07-28T09:51:28.780352Z","iopub.execute_input":"2024-07-28T09:51:28.780661Z","iopub.status.idle":"2024-07-28T09:51:40.859592Z","shell.execute_reply.started":"2024-07-28T09:51:28.780633Z","shell.execute_reply":"2024-07-28T09:51:40.858661Z"},"trusted":true},"execution_count":6,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtokenized_dataset\u001b[49m\n","\u001b[0;31mNameError\u001b[0m: name 'tokenized_dataset' is not defined"],"ename":"NameError","evalue":"name 'tokenized_dataset' is not defined","output_type":"error"}]},{"cell_type":"code","source":"tokenizer = GPT2Tokenizer.from_pretrained('codeparrot/codeparrot')\ntokenizer.add_special_tokens({'pad_token': '[PAD]'})\ndata_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors=\"tf\")","metadata":{"execution":{"iopub.status.busy":"2024-07-28T05:47:55.821133Z","iopub.execute_input":"2024-07-28T05:47:55.821530Z","iopub.status.idle":"2024-07-28T05:47:56.774307Z","shell.execute_reply.started":"2024-07-28T05:47:55.821500Z","shell.execute_reply":"2024-07-28T05:47:56.773244Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"tf_train_dataset =tokenized_dataset['train'].to_tf_dataset(\ncolumns=[\"input_ids\"],\n    label_cols=[\"labels\"],\n    collate_fn = data_collator,\n    shuffle = True,\n    batch_size = 16\n    \n)","metadata":{"execution":{"iopub.status.busy":"2024-07-28T05:48:21.694785Z","iopub.execute_input":"2024-07-28T05:48:21.695219Z","iopub.status.idle":"2024-07-28T05:48:22.001288Z","shell.execute_reply.started":"2024-07-28T05:48:21.695186Z","shell.execute_reply":"2024-07-28T05:48:22.000299Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/site-packages/datasets/arrow_dataset.py:410: FutureWarning: The output of `to_tf_dataset` will change when a passing single element list for `labels` or `columns` in the next datasets version. To return a tuple structure rather than dict, pass a single string.\nOld behaviour: columns=['a'], labels=['labels'] -> (tf.Tensor, tf.Tensor)  \n             : columns='a', labels='labels' -> (tf.Tensor, tf.Tensor)  \nNew behaviour: columns=['a'],labels=['labels'] -> ({'a': tf.Tensor}, {'labels': tf.Tensor})  \n             : columns='a', labels='labels' -> (tf.Tensor, tf.Tensor) \n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"from tensorflow.keras.losses import SparseCategoricalCrossentropy\nwith strategy.scope():\n#     model = TFGPT2LMHeadModel.from_pretrained('gpt2-large')\n#     loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n#     model.compile(optimizer='adam', loss=loss, metrics=[\"accuracy\"])\n#     model.fit(\n#         tf_train_dataset, epochs=1\n#     )\n    model.push_to_hub('gpt2-large-code',token = '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')","metadata":{"execution":{"iopub.status.busy":"2024-07-28T09:51:07.656803Z","iopub.execute_input":"2024-07-28T09:51:07.657421Z","iopub.status.idle":"2024-07-28T09:51:11.422617Z","shell.execute_reply.started":"2024-07-28T09:51:07.657388Z","shell.execute_reply":"2024-07-28T09:51:11.421712Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stderr","text":"WARNING: Logging before InitGoogle() is written to STDERR\nE0000 00:00:1722160268.355579    4394 common_lib.cc:798] Could not set metric server port: INVALID_ARGUMENT: Could not find SliceBuilder port 8471 in any of the 0 ports provided in `tpu_process_addresses`=\"local\"\n=== Source Location Trace: === \nlearning/45eac/tfrc/runtime/common_lib.cc:479\nD0728 09:51:08.364717549    4394 config.cc:196]                        gRPC EXPERIMENT call_status_override_on_cancellation   OFF (default:OFF)\nD0728 09:51:08.364735819    4394 config.cc:196]                        gRPC EXPERIMENT call_v3                                OFF (default:OFF)\nD0728 09:51:08.364739404    4394 config.cc:196]                        gRPC EXPERIMENT canary_client_privacy                  ON  (default:ON)\nD0728 09:51:08.364742073    4394 config.cc:196]                        gRPC EXPERIMENT capture_base_context                   ON  (default:ON)\nD0728 09:51:08.364744673    4394 config.cc:196]                        gRPC EXPERIMENT client_idleness                        ON  (default:ON)\nD0728 09:51:08.364747318    4394 config.cc:196]                        gRPC EXPERIMENT client_privacy                         ON  (default:ON)\nD0728 09:51:08.364749999    4394 config.cc:196]                        gRPC EXPERIMENT dapper_request_wire_size               OFF (default:OFF)\nD0728 09:51:08.364752476    4394 config.cc:196]                        gRPC EXPERIMENT empty_experiment                       OFF (default:OFF)\nD0728 09:51:08.364754948    4394 config.cc:196]                        gRPC EXPERIMENT event_engine_client                    OFF (default:OFF)\nD0728 09:51:08.364757416    4394 config.cc:196]                        gRPC EXPERIMENT event_engine_dns                       ON  (default:ON)\nD0728 09:51:08.364760044    4394 config.cc:196]                        gRPC EXPERIMENT event_engine_listener                  ON  (default:ON)\nD0728 09:51:08.364762551    4394 config.cc:196]                        gRPC EXPERIMENT free_large_allocator                   OFF (default:OFF)\nD0728 09:51:08.364765005    4394 config.cc:196]                        gRPC EXPERIMENT google_no_envelope_resolver            OFF (default:OFF)\nD0728 09:51:08.364767470    4394 config.cc:196]                        gRPC EXPERIMENT http2_stats_fix                        OFF (default:OFF)\nD0728 09:51:08.364769949    4394 config.cc:196]                        gRPC EXPERIMENT keepalive_fix                          OFF (default:OFF)\nD0728 09:51:08.364772463    4394 config.cc:196]                        gRPC EXPERIMENT keepalive_server_fix                   ON  (default:ON)\nD0728 09:51:08.364775119    4394 config.cc:196]                        gRPC EXPERIMENT loas_do_not_prefer_rekey_next_protocol OFF (default:OFF)\nD0728 09:51:08.364777650    4394 config.cc:196]                        gRPC EXPERIMENT loas_prod_to_cloud_prefer_pfs_ciphers  OFF (default:OFF)\nD0728 09:51:08.364780147    4394 config.cc:196]                        gRPC EXPERIMENT monitoring_experiment                  ON  (default:ON)\nD0728 09:51:08.364782695    4394 config.cc:196]                        gRPC EXPERIMENT multiping                              OFF (default:OFF)\nD0728 09:51:08.364785191    4394 config.cc:196]                        gRPC EXPERIMENT peer_state_based_framing               OFF (default:OFF)\nD0728 09:51:08.364787675    4394 config.cc:196]                        gRPC EXPERIMENT pending_queue_cap                      ON  (default:ON)\nD0728 09:51:08.364790295    4394 config.cc:196]                        gRPC EXPERIMENT pick_first_happy_eyeballs              ON  (default:ON)\nD0728 09:51:08.364792816    4394 config.cc:196]                        gRPC EXPERIMENT promise_based_client_call              OFF (default:OFF)\nD0728 09:51:08.364795263    4394 config.cc:196]                        gRPC EXPERIMENT promise_based_inproc_transport         OFF (default:OFF)\nD0728 09:51:08.364797719    4394 config.cc:196]                        gRPC EXPERIMENT promise_based_server_call              OFF (default:OFF)\nD0728 09:51:08.364800287    4394 config.cc:196]                        gRPC EXPERIMENT registered_method_lookup_in_transport  ON  (default:ON)\nD0728 09:51:08.364802795    4394 config.cc:196]                        gRPC EXPERIMENT rfc_max_concurrent_streams             ON  (default:ON)\nD0728 09:51:08.364805436    4394 config.cc:196]                        gRPC EXPERIMENT round_robin_delegate_to_pick_first     ON  (default:ON)\nD0728 09:51:08.364809143    4394 config.cc:196]                        gRPC EXPERIMENT rstpit                                 OFF (default:OFF)\nD0728 09:51:08.364812195    4394 config.cc:196]                        gRPC EXPERIMENT schedule_cancellation_over_write       OFF (default:OFF)\nD0728 09:51:08.364814842    4394 config.cc:196]                        gRPC EXPERIMENT server_privacy                         ON  (default:ON)\nD0728 09:51:08.364817531    4394 config.cc:196]                        gRPC EXPERIMENT tcp_frame_size_tuning                  OFF (default:OFF)\nD0728 09:51:08.364820025    4394 config.cc:196]                        gRPC EXPERIMENT tcp_rcv_lowat                          OFF (default:OFF)\nD0728 09:51:08.364822497    4394 config.cc:196]                        gRPC EXPERIMENT trace_record_callops                   OFF (default:OFF)\nD0728 09:51:08.364824959    4394 config.cc:196]                        gRPC EXPERIMENT unconstrained_max_quota_buffer_size    OFF (default:OFF)\nD0728 09:51:08.364827375    4394 config.cc:196]                        gRPC EXPERIMENT v3_backend_metric_filter               OFF (default:OFF)\nD0728 09:51:08.364829833    4394 config.cc:196]                        gRPC EXPERIMENT v3_channel_idle_filters                ON  (default:ON)\nD0728 09:51:08.364832324    4394 config.cc:196]                        gRPC EXPERIMENT v3_compression_filter                  ON  (default:ON)\nD0728 09:51:08.364834809    4394 config.cc:196]                        gRPC EXPERIMENT v3_server_auth_filter                  OFF (default:OFF)\nD0728 09:51:08.364837245    4394 config.cc:196]                        gRPC EXPERIMENT work_serializer_clears_time_cache      OFF (default:OFF)\nD0728 09:51:08.364839677    4394 config.cc:196]                        gRPC EXPERIMENT work_serializer_dispatch               OFF (default:OFF)\nD0728 09:51:08.364842173    4394 config.cc:196]                        gRPC EXPERIMENT write_size_cap                         ON  (default:ON)\nD0728 09:51:08.364844682    4394 config.cc:196]                        gRPC EXPERIMENT write_size_policy                      ON  (default:ON)\nD0728 09:51:08.364847240    4394 config.cc:196]                        gRPC EXPERIMENT wrr_delegate_to_pick_first             ON  (default:ON)\nI0728 09:51:08.365062202    4394 ev_epoll1_linux.cc:123]               grpc epoll fd: 59\nD0728 09:51:08.365077123    4394 ev_posix.cc:113]                      Using polling engine: epoll1\nD0728 09:51:08.376120929    4394 lb_policy_registry.cc:46]             registering LB policy factory for \"priority_experimental\"\nD0728 09:51:08.376134210    4394 lb_policy_registry.cc:46]             registering LB policy factory for \"outlier_detection_experimental\"\nD0728 09:51:08.376145545    4394 lb_policy_registry.cc:46]             registering LB policy factory for \"weighted_target_experimental\"\nD0728 09:51:08.376151085    4394 lb_policy_registry.cc:46]             registering LB policy factory for \"pick_first\"\nD0728 09:51:08.376173443    4394 lb_policy_registry.cc:46]             registering LB policy factory for \"round_robin\"\nD0728 09:51:08.376178467    4394 lb_policy_registry.cc:46]             registering LB policy factory for \"weighted_round_robin\"\nD0728 09:51:08.376229672    4394 lb_policy_registry.cc:46]             registering LB policy factory for \"grpclb\"\nD0728 09:51:08.376256703    4394 dns_resolver_plugin.cc:43]            Using EventEngine dns resolver\nD0728 09:51:08.376287595    4394 lb_policy_registry.cc:46]             registering LB policy factory for \"rls_experimental\"\nD0728 09:51:08.376334219    4394 lb_policy_registry.cc:46]             registering LB policy factory for \"xds_cluster_manager_experimental\"\nD0728 09:51:08.376345817    4394 lb_policy_registry.cc:46]             registering LB policy factory for \"xds_cluster_impl_experimental\"\nD0728 09:51:08.376351150    4394 lb_policy_registry.cc:46]             registering LB policy factory for \"cds_experimental\"\nD0728 09:51:08.376357933    4394 lb_policy_registry.cc:46]             registering LB policy factory for \"xds_override_host_experimental\"\nD0728 09:51:08.376363420    4394 lb_policy_registry.cc:46]             registering LB policy factory for \"xds_wrr_locality_experimental\"\nD0728 09:51:08.376371071    4394 lb_policy_registry.cc:46]             registering LB policy factory for \"ring_hash_experimental\"\nD0728 09:51:08.376376647    4394 certificate_provider_registry.cc:33]  registering certificate provider factory for \"file_watcher\"\nD0728 09:51:08.376419255    4394 channel_init.cc:157]                  Filter server-auth not registered, but is referenced in the after clause of grpc-server-authz when building channel stack SERVER_CHANNEL\nI0728 09:51:08.378264455    4394 ev_epoll1_linux.cc:359]               grpc epoll fd: 61\nI0728 09:51:08.379655423    4394 tcp_socket_utils.cc:689]              Disabling AF_INET6 sockets because ::1 is not available.\nI0728 09:51:08.420104356    4485 socket_utils_common_posix.cc:452]     Disabling AF_INET6 sockets because ::1 is not available.\nI0728 09:51:08.420153603    4485 socket_utils_common_posix.cc:379]     TCP_USER_TIMEOUT is available. TCP_USER_TIMEOUT will be used thereafter\nE0728 09:51:08.427360214    4394 oauth2_credentials.cc:238]            oauth_fetch: UNKNOWN:C-ares status is not ARES_SUCCESS qtype=A name=metadata.google.internal. is_balancer=0: Domain name not found {created_time:\"2024-07-28T09:51:08.427339562+00:00\", grpc_status:2}\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[4], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlosses\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SparseCategoricalCrossentropy\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mstrategy\u001b[49m\u001b[38;5;241m.\u001b[39mscope():\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m#     model = TFGPT2LMHeadModel.from_pretrained('gpt2-large')\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m#     loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m#     model.compile(optimizer='adam', loss=loss, metrics=[\"accuracy\"])\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m#     model.fit(\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m#         tf_train_dataset, epochs=1\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m#     )\u001b[39;00m\n\u001b[1;32m      9\u001b[0m     model\u001b[38;5;241m.\u001b[39mpush_to_hub(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgpt2-large-code\u001b[39m\u001b[38;5;124m'\u001b[39m,token \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhf_xXxEplqnxGTVmSxcHzEDHorSSFxKAGjtZH\u001b[39m\u001b[38;5;124m'\u001b[39m)\n","\u001b[0;31mNameError\u001b[0m: name 'strategy' is not defined"],"ename":"NameError","evalue":"name 'strategy' is not defined","output_type":"error"}]},{"cell_type":"code","source":"model.push_to_hub('gpt2-large-code',token = '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')","metadata":{"execution":{"iopub.status.busy":"2024-07-28T09:52:18.817648Z","iopub.execute_input":"2024-07-28T09:52:18.818031Z","iopub.status.idle":"2024-07-28T09:52:18.842510Z","shell.execute_reply.started":"2024-07-28T09:52:18.817985Z","shell.execute_reply":"2024-07-28T09:52:18.841704Z"},"trusted":true},"execution_count":8,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39mpush_to_hub(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgpt2-large-code\u001b[39m\u001b[38;5;124m'\u001b[39m,token \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhf_xXxEplqnxGTVmSxcHzEDHorSSFxKAGjtZH\u001b[39m\u001b[38;5;124m'\u001b[39m)\n","\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"],"ename":"NameError","evalue":"name 'model' is not defined","output_type":"error"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}